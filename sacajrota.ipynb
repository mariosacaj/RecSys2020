{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {},
    "colab_type": "code",
    "id": "7Ifkd08Lq3wR",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import csv\n",
    "import scipy as sp\n",
    "import scipy.sparse as sps\n",
    "from scipy.sparse import coo_matrix\n",
    "import os\n",
    "\n",
    "## Evaluation or scoring?\n",
    "eval = False\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "file = []\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    #for filename in filenames:\n",
    "        #file.append(os.path.join(dirname, filename))\n",
    "# print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6mDejDEXq3wW",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Visualize rows/columns stats\n",
    "def list_ID_stats(ID_list, label):\n",
    "    ID_list = list(map(int, ID_list))\n",
    "    list_length = len(ID_list)\n",
    "    min_val = min(ID_list)\n",
    "    max_val = max(ID_list)\n",
    "    unique_val = len(set(ID_list))\n",
    "    repetitions = list_length - unique_val\n",
    "    delta = max_val - min_val\n",
    "    missing_val = 0.\n",
    "    if delta is not 0:\n",
    "        missing_val = 1 - min(unique_val, delta)/delta\n",
    "\n",
    "    print(\"{} data, ID: min {}, max {}, length {}, unique {}, repetitions {}, missig {:.2f} %\".format(label, min_val, max_val, list_length, unique_val, repetitions, missing_val*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "unO7P8Ezq3wY",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# This function loads CSV files to COOrdinate formatted sparse matrixes\n",
    "def toCoo(filepath, rowsDesc, columnsDesc):\n",
    "    rows = []\n",
    "    columns = []\n",
    "    data = []\n",
    "    with open(filepath) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if line_count == 0:\n",
    "                line_count += 1\n",
    "            else:\n",
    "                rows.append(row[0])\n",
    "                columns.append(row[1])\n",
    "                data.append(row[2])\n",
    "                line_count += 1\n",
    "    print(filepath)\n",
    "    list_ID_stats(rows, rowsDesc)\n",
    "    list_ID_stats(columns, columnsDesc)\n",
    "    print(\n",
    "    )\n",
    "    data = np.array(data).astype(np.float)\n",
    "    rows = np.array(rows).astype(np.int)\n",
    "    columns = np.array(columns).astype(np.int)\n",
    "    return coo_matrix((data, (rows, columns)))\n",
    "\n",
    "# This function loads CSV files to NParrays\n",
    "def toNPArray(filepath):\n",
    "    users = []\n",
    "    with open(filepath) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if line_count == 0:\n",
    "                line_count += 1\n",
    "            else:\n",
    "                users.append(row[0])\n",
    "                line_count += 1\n",
    "    users = np.array(users).astype(np.int)\n",
    "    return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "eWpETPwzq3wa",
    "outputId": "4825aae1-1614-4865-a19a-7b23f32e16b9",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_UCM_age.csv\n",
      "user data, ID: min 2, max 30909, length 30317, unique 30317, repetitions 0, missig 1.91 %\n",
      "age data, ID: min 1, max 10, length 30317, unique 10, repetitions 30307, missig 0.00 %\n",
      "\n",
      "data_ICM_sub_class.csv\n",
      "item data, ID: min 0, max 18494, length 18495, unique 18495, repetitions 0, missig 0.00 %\n",
      "subclass data, ID: min 1, max 2010, length 18495, unique 1905, repetitions 16590, missig 5.18 %\n",
      "\n",
      "data_ICM_asset.csv\n",
      "item data, ID: min 0, max 18494, length 18490, unique 18490, repetitions 0, missig 0.02 %\n",
      "asset data, ID: min 0, max 0, length 18490, unique 1, repetitions 18489, missig 0.00 %\n",
      "\n",
      "data_ICM_price.csv\n",
      "item data, ID: min 0, max 18494, length 18493, unique 18493, repetitions 0, missig 0.01 %\n",
      "price data, ID: min 0, max 0, length 18493, unique 1, repetitions 18492, missig 0.00 %\n",
      "\n",
      "data_UCM_region.csv\n",
      "user data, ID: min 0, max 30910, length 26609, unique 26375, repetitions 234, missig 14.67 %\n",
      "region data, ID: min 0, max 7, length 26609, unique 7, repetitions 26602, missig 0.00 %\n",
      "\n",
      "data_train.csv\n",
      "user data, ID: min 0, max 30910, length 398636, unique 27255, repetitions 371381, missig 11.82 %\n",
      "item data, ID: min 0, max 18494, length 398636, unique 15277, repetitions 383359, missig 17.39 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading from CSV files...\n",
    "UCM_age = toCoo('data_UCM_age.csv', 'user', 'age')\n",
    "ICM_subclass = toCoo('data_ICM_sub_class.csv', 'item', 'subclass')\n",
    "ICM_asset = toCoo('data_ICM_asset.csv', 'item', 'asset')\n",
    "ICM_price = toCoo('data_ICM_price.csv', 'item', 'price')\n",
    "UCM_region = toCoo('data_UCM_region.csv', 'user', 'region')\n",
    "target_users = toNPArray('data_target_users_test.csv')\n",
    "URM = toCoo('data_train.csv', 'user', 'item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m4FzKl-xq3wc",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "URM = URM.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xUh-0q9Uq3we"
   },
   "source": [
    "From here down a CF approach is presented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fEalJLq-q3wf",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing for EVALUATING PURPOSES ONLY\n",
    "# Cold items have no impact in the evaluation, since they have no interactions\n",
    "if eval:  \n",
    "  warm_items_mask = np.ediff1d(URM.tocsc().indptr) > 0\n",
    "  warm_items = np.arange(URM.shape[1])[warm_items_mask]\n",
    "\n",
    "  URM = URM[:, warm_items]\n",
    "  URM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CFC6Ryh8q3wj",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing for EVALUATING PURPOSES ONLY\n",
    "# same holds for users\n",
    "if eval:\n",
    "  warm_users_mask = np.ediff1d(URM.tocsr().indptr) > 0\n",
    "  warm_users = np.arange(URM.shape[0])[warm_users_mask]\n",
    "\n",
    "  URM = URM[warm_users, :]\n",
    "  URM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2pMRUfamq3wl"
   },
   "source": [
    "Be careful! With this operation we lost the original mapping with item and user IDs!\n",
    "Keep the warm_items and warm_users array, we might need them in future..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wNt-YbcLtXCx",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "### Data splitting ###\n",
    "def train_test_holdout(URM_all, train_perc = 0.8):\n",
    "\n",
    "\n",
    "    numInteractions = URM_all.nnz\n",
    "    URM_all = URM_all.tocoo()\n",
    "    shape = URM_all.shape\n",
    "\n",
    "\n",
    "    train_mask = np.random.choice([True,False], numInteractions, p=[train_perc, 1-train_perc])\n",
    "\n",
    "\n",
    "    URM_train = sps.coo_matrix((URM_all.data[train_mask], (URM_all.row[train_mask], URM_all.col[train_mask])), shape=shape)\n",
    "    URM_train = URM_train.tocsr()\n",
    "\n",
    "    test_mask = np.logical_not(train_mask)\n",
    "\n",
    "    URM_test = sps.coo_matrix((URM_all.data[test_mask], (URM_all.row[test_mask], URM_all.col[test_mask])), shape=shape)\n",
    "    URM_test = URM_test.tocsr()\n",
    "\n",
    "    return URM_train, URM_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "onJ5STbFzKFg",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "### Evaluation ###\n",
    "def precision(is_relevant, relevant_items):\n",
    "\n",
    "    #is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "\n",
    "    precision_score = np.sum(is_relevant, dtype=np.float32) / len(is_relevant)\n",
    "\n",
    "    return precision_score\n",
    "\n",
    "\n",
    "\n",
    "def recall(is_relevant, relevant_items):\n",
    "\n",
    "    #is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "\n",
    "    recall_score = np.sum(is_relevant, dtype=np.float32) / relevant_items.shape[0]\n",
    "\n",
    "    return recall_score\n",
    "\n",
    "\n",
    "\n",
    "def MAP(is_relevant, relevant_items):\n",
    "\n",
    "    #is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "\n",
    "    # Cumulative sum: precision at 1, at 2, at 3 ...\n",
    "    p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
    "\n",
    "    map_score = np.sum(p_at_k) / np.min([relevant_items.shape[0], is_relevant.shape[0]])\n",
    "\n",
    "    return map_score\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_algorithm(URM_test, recommender_object, at=5):\n",
    "\n",
    "    cumulative_precision = 0.0\n",
    "    cumulative_recall = 0.0\n",
    "    cumulative_MAP = 0.0\n",
    "\n",
    "    num_eval = 0\n",
    "\n",
    "    URM_test = sps.csr_matrix(URM_test)\n",
    "\n",
    "    n_users = URM_test.shape[0]\n",
    "\n",
    "\n",
    "    for user_id in range(n_users):\n",
    "\n",
    "        if user_id % 10000 == 0:\n",
    "            print(\"Evaluated user {} of {}\".format(user_id, n_users))\n",
    "\n",
    "        start_pos = URM_test.indptr[user_id]\n",
    "        end_pos = URM_test.indptr[user_id+1]\n",
    "\n",
    "        if end_pos-start_pos>0:\n",
    "\n",
    "            relevant_items = URM_test.indices[start_pos:end_pos]\n",
    "\n",
    "            recommended_items = recommender_object.recommend(user_id, at=at)\n",
    "            num_eval+=1\n",
    "\n",
    "            is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "\n",
    "            cumulative_precision += precision(is_relevant, relevant_items)\n",
    "            cumulative_recall += recall(is_relevant, relevant_items)\n",
    "            cumulative_MAP += MAP(is_relevant, relevant_items)\n",
    "\n",
    "\n",
    "    cumulative_precision /= num_eval\n",
    "    cumulative_recall /= num_eval\n",
    "    cumulative_MAP /= num_eval\n",
    "\n",
    "    print(\"Recommender performance is: Precision = {:.4f}, Recall = {:.4f}, MAP = {:.4f}\".format(\n",
    "        cumulative_precision, cumulative_recall, cumulative_MAP))\n",
    "\n",
    "    result_dict = {\n",
    "        \"precision\": cumulative_precision,\n",
    "        \"recall\": cumulative_recall,\n",
    "        \"MAP\": cumulative_MAP,\n",
    "    }\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qgq0M-Nq0212",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "### check matrix\n",
    "\n",
    "def check_matrix(X, format='csc', dtype=np.float32):\n",
    "    \"\"\"\n",
    "    This function takes a matrix as input and transforms it into the specified format.\n",
    "    The matrix in input can be either sparse or ndarray.\n",
    "    If the matrix in input has already the desired format, it is returned as-is\n",
    "    the dtype parameter is always applied and the default is np.float32\n",
    "    :param X:\n",
    "    :param format:\n",
    "    :param dtype:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if format == 'csc' and not isinstance(X, sps.csc_matrix):\n",
    "        return X.tocsc().astype(dtype)\n",
    "    elif format == 'csr' and not isinstance(X, sps.csr_matrix):\n",
    "        return X.tocsr().astype(dtype)\n",
    "    elif format == 'coo' and not isinstance(X, sps.coo_matrix):\n",
    "        return X.tocoo().astype(dtype)\n",
    "    elif format == 'dok' and not isinstance(X, sps.dok_matrix):\n",
    "        return X.todok().astype(dtype)\n",
    "    elif format == 'bsr' and not isinstance(X, sps.bsr_matrix):\n",
    "        return X.tobsr().astype(dtype)\n",
    "    elif format == 'dia' and not isinstance(X, sps.dia_matrix):\n",
    "        return X.todia().astype(dtype)\n",
    "    elif format == 'lil' and not isinstance(X, sps.lil_matrix):\n",
    "        return X.tolil().astype(dtype)\n",
    "\n",
    "    elif format == 'npy':\n",
    "        if sps.issparse(X):\n",
    "            return X.toarray().astype(dtype)\n",
    "        else:\n",
    "            return np.array(X)\n",
    "\n",
    "    elif isinstance(X, np.ndarray):\n",
    "        X = sps.csr_matrix(X, dtype=dtype)\n",
    "        X.eliminate_zeros()\n",
    "        return check_matrix(X, format=format, dtype=dtype)\n",
    "    else:\n",
    "        return X.astype(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MWSuOwuP0jh0",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "### Compute Similarity Class\n",
    "\n",
    "import time, sys\n",
    "\n",
    "class Compute_Similarity_Python:\n",
    "\n",
    "\n",
    "    def __init__(self, dataMatrix, topK=100, shrink = 0, normalize = True,\n",
    "                 asymmetric_alpha = 0.5, tversky_alpha = 1.0, tversky_beta = 1.0,\n",
    "                 similarity = \"cosine\", row_weights = None):\n",
    "        \"\"\"\n",
    "        Computes the cosine similarity on the columns of dataMatrix\n",
    "        If it is computed on URM=|users|x|items|, pass the URM as is.\n",
    "        If it is computed on ICM=|items|x|features|, pass the ICM transposed.\n",
    "        :param dataMatrix:\n",
    "        :param topK:\n",
    "        :param shrink:\n",
    "        :param normalize:           If True divide the dot product by the product of the norms\n",
    "        :param row_weights:         Multiply the values in each row by a specified value. Array\n",
    "        :param asymmetric_alpha     Coefficient alpha for the asymmetric cosine\n",
    "        :param similarity:  \"cosine\"        computes Cosine similarity\n",
    "                            \"adjusted\"      computes Adjusted Cosine, removing the average of the users\n",
    "                            \"asymmetric\"    computes Asymmetric Cosine\n",
    "                            \"pearson\"       computes Pearson Correlation, removing the average of the items\n",
    "                            \"jaccard\"       computes Jaccard similarity for binary interactions using Tanimoto\n",
    "                            \"dice\"          computes Dice similarity for binary interactions\n",
    "                            \"tversky\"       computes Tversky similarity for binary interactions\n",
    "                            \"tanimoto\"      computes Tanimoto coefficient for binary interactions\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Asymmetric Cosine as described in: \n",
    "        Aiolli, F. (2013, October). Efficient top-n recommendation for very large scale binary rated datasets. In Proceedings of the 7th ACM conference on Recommender systems (pp. 273-280). ACM.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        super(Compute_Similarity_Python, self).__init__()\n",
    "\n",
    "\n",
    "        self.shrink = shrink\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self.n_rows, self.n_columns = dataMatrix.shape\n",
    "        self.TopK = min(topK, self.n_columns)\n",
    "\n",
    "        self.asymmetric_alpha = asymmetric_alpha\n",
    "        self.tversky_alpha = tversky_alpha\n",
    "        self.tversky_beta = tversky_beta\n",
    "\n",
    "        self.dataMatrix = dataMatrix.copy()\n",
    "\n",
    "        self.adjusted_cosine = False\n",
    "        self.asymmetric_cosine = False\n",
    "        self.pearson_correlation = False\n",
    "        self.tanimoto_coefficient = False\n",
    "        self.dice_coefficient = False\n",
    "        self.tversky_coefficient = False\n",
    "\n",
    "        if similarity == \"adjusted\":\n",
    "            self.adjusted_cosine = True\n",
    "        elif similarity == \"asymmetric\":\n",
    "            self.asymmetric_cosine = True\n",
    "        elif similarity == \"pearson\":\n",
    "            self.pearson_correlation = True\n",
    "        elif similarity == \"jaccard\" or similarity == \"tanimoto\":\n",
    "            self.tanimoto_coefficient = True\n",
    "            # Tanimoto has a specific kind of normalization\n",
    "            self.normalize = False\n",
    "\n",
    "        elif similarity == \"dice\":\n",
    "            self.dice_coefficient = True\n",
    "            self.normalize = False\n",
    "\n",
    "        elif similarity == \"tversky\":\n",
    "            self.tversky_coefficient = True\n",
    "            self.normalize = False\n",
    "\n",
    "        elif similarity == \"cosine\":\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"Cosine_Similarity: value for parameter 'mode' not recognized.\"\n",
    "                             \" Allowed values are: 'cosine', 'pearson', 'adjusted', 'asymmetric', 'jaccard', 'tanimoto',\"\n",
    "                             \"dice, tversky.\"\n",
    "                             \" Passed value was '{}'\".format(similarity))\n",
    "\n",
    "\n",
    "        self.use_row_weights = False\n",
    "\n",
    "        if row_weights is not None:\n",
    "\n",
    "            if dataMatrix.shape[0] != len(row_weights):\n",
    "                raise ValueError(\"Cosine_Similarity: provided row_weights and dataMatrix have different number of rows.\"\n",
    "                                 \"Col_weights has {} columns, dataMatrix has {}.\".format(len(row_weights), dataMatrix.shape[0]))\n",
    "\n",
    "            self.use_row_weights = True\n",
    "            self.row_weights = row_weights.copy()\n",
    "            self.row_weights_diag = sps.diags(self.row_weights)\n",
    "\n",
    "            self.dataMatrix_weighted = self.dataMatrix.T.dot(self.row_weights_diag).T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def applyAdjustedCosine(self):\n",
    "        \"\"\"\n",
    "        Remove from every data point the average for the corresponding row\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        self.dataMatrix = check_matrix(self.dataMatrix, 'csr')\n",
    "\n",
    "\n",
    "        interactionsPerRow = np.diff(self.dataMatrix.indptr)\n",
    "\n",
    "        nonzeroRows = interactionsPerRow > 0\n",
    "        sumPerRow = np.asarray(self.dataMatrix.sum(axis=1)).ravel()\n",
    "\n",
    "        rowAverage = np.zeros_like(sumPerRow)\n",
    "        rowAverage[nonzeroRows] = sumPerRow[nonzeroRows] / interactionsPerRow[nonzeroRows]\n",
    "\n",
    "\n",
    "        # Split in blocks to avoid duplicating the whole data structure\n",
    "        start_row = 0\n",
    "        end_row= 0\n",
    "\n",
    "        blockSize = 1000\n",
    "\n",
    "\n",
    "        while end_row < self.n_rows:\n",
    "\n",
    "            end_row = min(self.n_rows, end_row + blockSize)\n",
    "\n",
    "            self.dataMatrix.data[self.dataMatrix.indptr[start_row]:self.dataMatrix.indptr[end_row]] -= \\\n",
    "                np.repeat(rowAverage[start_row:end_row], interactionsPerRow[start_row:end_row])\n",
    "\n",
    "            start_row += blockSize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def applyPearsonCorrelation(self):\n",
    "        \"\"\"\n",
    "        Remove from every data point the average for the corresponding column\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n",
    "\n",
    "\n",
    "        interactionsPerCol = np.diff(self.dataMatrix.indptr)\n",
    "\n",
    "        nonzeroCols = interactionsPerCol > 0\n",
    "        sumPerCol = np.asarray(self.dataMatrix.sum(axis=0)).ravel()\n",
    "\n",
    "        colAverage = np.zeros_like(sumPerCol)\n",
    "        colAverage[nonzeroCols] = sumPerCol[nonzeroCols] / interactionsPerCol[nonzeroCols]\n",
    "\n",
    "\n",
    "        # Split in blocks to avoid duplicating the whole data structure\n",
    "        start_col = 0\n",
    "        end_col= 0\n",
    "\n",
    "        blockSize = 1000\n",
    "\n",
    "\n",
    "        while end_col < self.n_columns:\n",
    "\n",
    "            end_col = min(self.n_columns, end_col + blockSize)\n",
    "\n",
    "            self.dataMatrix.data[self.dataMatrix.indptr[start_col]:self.dataMatrix.indptr[end_col]] -= \\\n",
    "                np.repeat(colAverage[start_col:end_col], interactionsPerCol[start_col:end_col])\n",
    "\n",
    "            start_col += blockSize\n",
    "\n",
    "\n",
    "    def useOnlyBooleanInteractions(self):\n",
    "\n",
    "        # Split in blocks to avoid duplicating the whole data structure\n",
    "        start_pos = 0\n",
    "        end_pos= 0\n",
    "\n",
    "        blockSize = 1000\n",
    "\n",
    "\n",
    "        while end_pos < len(self.dataMatrix.data):\n",
    "\n",
    "            end_pos = min(len(self.dataMatrix.data), end_pos + blockSize)\n",
    "\n",
    "            self.dataMatrix.data[start_pos:end_pos] = np.ones(end_pos-start_pos)\n",
    "\n",
    "            start_pos += blockSize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def compute_similarity(self, start_col=None, end_col=None, block_size = 100):\n",
    "        \"\"\"\n",
    "        Compute the similarity for the given dataset\n",
    "        :param self:\n",
    "        :param start_col: column to begin with\n",
    "        :param end_col: column to stop before, end_col is excluded\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        values = []\n",
    "        rows = []\n",
    "        cols = []\n",
    "\n",
    "        start_time = time.time()\n",
    "        start_time_print_batch = start_time\n",
    "        processedItems = 0\n",
    "\n",
    "\n",
    "        if self.adjusted_cosine:\n",
    "            self.applyAdjustedCosine()\n",
    "\n",
    "        elif self.pearson_correlation:\n",
    "            self.applyPearsonCorrelation()\n",
    "\n",
    "        elif self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient:\n",
    "            self.useOnlyBooleanInteractions()\n",
    "\n",
    "\n",
    "        # We explore the matrix column-wise\n",
    "        self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n",
    "\n",
    "\n",
    "        # Compute sum of squared values to be used in normalization\n",
    "        sumOfSquared = np.array(self.dataMatrix.power(2).sum(axis=0)).ravel()\n",
    "\n",
    "        # Tanimoto does not require the square root to be applied\n",
    "        if not (self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient):\n",
    "            sumOfSquared = np.sqrt(sumOfSquared)\n",
    "\n",
    "        if self.asymmetric_cosine:\n",
    "            sumOfSquared_to_1_minus_alpha = np.power(sumOfSquared, 2 * (1 - self.asymmetric_alpha))\n",
    "            sumOfSquared_to_alpha = np.power(sumOfSquared, 2 * self.asymmetric_alpha)\n",
    "\n",
    "\n",
    "        self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n",
    "\n",
    "        start_col_local = 0\n",
    "        end_col_local = self.n_columns\n",
    "\n",
    "        if start_col is not None and start_col>0 and start_col<self.n_columns:\n",
    "            start_col_local = start_col\n",
    "\n",
    "        if end_col is not None and end_col>start_col_local and end_col<self.n_columns:\n",
    "            end_col_local = end_col\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        start_col_block = start_col_local\n",
    "\n",
    "        this_block_size = 0\n",
    "\n",
    "        # Compute all similarities for each item using vectorization\n",
    "        while start_col_block < end_col_local:\n",
    "\n",
    "\n",
    "            end_col_block = min(start_col_block + block_size, end_col_local)\n",
    "            this_block_size = end_col_block-start_col_block\n",
    "\n",
    "\n",
    "\n",
    "            # All data points for a given item\n",
    "            item_data = self.dataMatrix[:, start_col_block:end_col_block]\n",
    "            item_data = item_data.toarray().squeeze()\n",
    "\n",
    "            # If only 1 feature avoid last dimension to disappear\n",
    "            if item_data.ndim == 1:\n",
    "                item_data = np.atleast_2d(item_data)\n",
    "\n",
    "            if self.use_row_weights:\n",
    "                this_block_weights = self.dataMatrix_weighted.T.dot(item_data)\n",
    "\n",
    "            else:\n",
    "                # Compute item similarities\n",
    "                this_block_weights = self.dataMatrix.T.dot(item_data)\n",
    "\n",
    "\n",
    "\n",
    "            for col_index_in_block in range(this_block_size):\n",
    "\n",
    "                if this_block_size == 1:\n",
    "                    this_column_weights = this_block_weights\n",
    "                else:\n",
    "                    this_column_weights = this_block_weights[:,col_index_in_block]\n",
    "\n",
    "\n",
    "                columnIndex = col_index_in_block + start_col_block\n",
    "                this_column_weights[columnIndex] = 0.0\n",
    "\n",
    "                # Apply normalization and shrinkage, ensure denominator != 0\n",
    "                if self.normalize:\n",
    "\n",
    "                    if self.asymmetric_cosine:\n",
    "                        denominator = sumOfSquared_to_alpha[columnIndex] * sumOfSquared_to_1_minus_alpha + self.shrink + 1e-6\n",
    "                    else:\n",
    "                        denominator = sumOfSquared[columnIndex] * sumOfSquared + self.shrink + 1e-6\n",
    "\n",
    "                    this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n",
    "\n",
    "\n",
    "                # Apply the specific denominator for Tanimoto\n",
    "                elif self.tanimoto_coefficient:\n",
    "                    denominator = sumOfSquared[columnIndex] + sumOfSquared - this_column_weights + self.shrink + 1e-6\n",
    "                    this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n",
    "\n",
    "                elif self.dice_coefficient:\n",
    "                    denominator = sumOfSquared[columnIndex] + sumOfSquared + self.shrink + 1e-6\n",
    "                    this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n",
    "\n",
    "                elif self.tversky_coefficient:\n",
    "                    denominator = this_column_weights + \\\n",
    "                                  (sumOfSquared[columnIndex] - this_column_weights)*self.tversky_alpha + \\\n",
    "                                  (sumOfSquared - this_column_weights)*self.tversky_beta + self.shrink + 1e-6\n",
    "                    this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n",
    "\n",
    "                # If no normalization or tanimoto is selected, apply only shrink\n",
    "                elif self.shrink != 0:\n",
    "                    this_column_weights = this_column_weights/self.shrink\n",
    "\n",
    "\n",
    "                #this_column_weights = this_column_weights.toarray().ravel()\n",
    "\n",
    "                # Sort indices and select TopK\n",
    "                # Sorting is done in three steps. Faster then plain np.argsort for higher number of items\n",
    "                # - Partition the data to extract the set of relevant items\n",
    "                # - Sort only the relevant items\n",
    "                # - Get the original item index\n",
    "                relevant_items_partition = (-this_column_weights).argpartition(self.TopK-1)[0:self.TopK]\n",
    "                relevant_items_partition_sorting = np.argsort(-this_column_weights[relevant_items_partition])\n",
    "                top_k_idx = relevant_items_partition[relevant_items_partition_sorting]\n",
    "\n",
    "                # Incrementally build sparse matrix, do not add zeros\n",
    "                notZerosMask = this_column_weights[top_k_idx] != 0.0\n",
    "                numNotZeros = np.sum(notZerosMask)\n",
    "\n",
    "                values.extend(this_column_weights[top_k_idx][notZerosMask])\n",
    "                rows.extend(top_k_idx[notZerosMask])\n",
    "                cols.extend(np.ones(numNotZeros) * columnIndex)\n",
    "\n",
    "\n",
    "            # Add previous block size\n",
    "            processedItems += this_block_size\n",
    "\n",
    "\n",
    "            if time.time() - start_time_print_batch >= 30 or end_col_block==end_col_local:\n",
    "                columnPerSec = processedItems / (time.time() - start_time + 1e-9)\n",
    "\n",
    "                print(\"Similarity column {} ( {:2.0f} % ), {:.2f} column/sec, elapsed time {:.2f} min\".format(\n",
    "                    processedItems, processedItems / (end_col_local - start_col_local) * 100, columnPerSec, (time.time() - start_time)/ 60))\n",
    "\n",
    "                sys.stdout.flush()\n",
    "                sys.stderr.flush()\n",
    "\n",
    "                start_time_print_batch = time.time()\n",
    "\n",
    "\n",
    "            start_col_block += block_size\n",
    "\n",
    "        # End while on columns\n",
    "\n",
    "        W_sparse = sps.csr_matrix((values, (rows, cols)),\n",
    "                                  shape=(self.n_columns, self.n_columns),\n",
    "                                  dtype=np.float32)\n",
    "\n",
    "\n",
    "        return W_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWufYR1j171U",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class ItemCFKNNRecommender(object):\n",
    "    \n",
    "    def __init__(self, URM):\n",
    "        self.URM = URM\n",
    "        \n",
    "            \n",
    "    def fit(self, topK=50, shrink=100, normalize=True, similarity=\"cosine\"):\n",
    "        \n",
    "        similarity_object = Compute_Similarity_Python(self.URM, shrink=shrink, \n",
    "                                                  topK=topK, normalize=normalize, \n",
    "                                                  similarity = similarity)\n",
    "        \n",
    "        self.W_sparse = similarity_object.compute_similarity()\n",
    "\n",
    "        \n",
    "    def recommend(self, user_id, at=None, exclude_seen=True):\n",
    "        # compute the scores using the dot product\n",
    "        user_profile = self.URM[user_id]\n",
    "        scores = user_profile.dot(self.W_sparse).toarray().ravel()\n",
    "\n",
    "        if exclude_seen:\n",
    "            scores = self.filter_seen(user_id, scores)\n",
    "\n",
    "        # rank items\n",
    "        ranking = scores.argsort()[::-1]\n",
    "            \n",
    "        return ranking[:at]\n",
    "    \n",
    "    \n",
    "    def filter_seen(self, user_id, scores):\n",
    "\n",
    "        start_pos = self.URM.indptr[user_id]\n",
    "        end_pos = self.URM.indptr[user_id+1]\n",
    "\n",
    "        user_profile = self.URM.indices[start_pos:end_pos]\n",
    "        \n",
    "        scores[user_profile] = -np.inf\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aODXsh3XsyJA",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Split\n",
    "URM_train, URM_test = train_test_holdout(URM, train_perc = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## SLIM BPR Recommender - MAP 0.41 on test set\n",
    "from SLIM_BPR.Cython.SLIM_BPR_Cython import SLIM_BPR_Cython\n",
    "from Base.Evaluation.Evaluator import EvaluatorHoldout\n",
    "import matplotlib.pyplot as pyplot\n",
    "%matplotlib inline  \n",
    "recommender = SLIM_BPR_Cython(URM_train, recompile_cython=False)\n",
    "MAP_LIST = []\n",
    "epochsList = [300]\n",
    "batchSize = [50]\n",
    "tklist = [10]\n",
    "lrs = [1e-3]\n",
    "\n",
    "for epochsN in epochsList:\n",
    "    for bs in batchSize:\n",
    "        for tk in tklist:\n",
    "            for lr in lrs:\n",
    "                recommender.fit(epochs=epochsN, batch_size=bs, sgd_mode='adagrad', learning_rate=lr, topK = tk)\n",
    "                evaluator_validation = EvaluatorHoldout(URM_test, cutoff_list=[10])\n",
    "                dict_scores = (evaluator_validation.evaluateRecommender(recommender))[0][10]\n",
    "                MAP_LIST.append(('epoch, batch, topK, lr :' + str(epochsN) + ' ' + str(bs)+ ' ' + str(tk) + ' ' + str(lr), dict_scores['MAP']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "Pm7oY2Zx6Soy",
    "outputId": "d56a6477-e677-4dbe-db85-49c119d4b348"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity column 18495 ( 100 % ), 1357.97 column/sec, elapsed time 0.23 min\n",
      "Evaluated user 0 of 30911\n",
      "Evaluated user 10000 of 30911\n",
      "Evaluated user 20000 of 30911\n",
      "Evaluated user 30000 of 30911\n",
      "Recommender performance is: Precision = 0.0080, Recall = 0.0400, MAP = 0.0250\n"
     ]
    }
   ],
   "source": [
    "## ItemCFKNNRecommender - MAP 0.47 on test set\n",
    "recommender = ItemCFKNNRecommender(URM_train)\n",
    "recommender.fit(shrink=8, topK=12)\n",
    "\n",
    "result_dict = evaluate_algorithm(URM_test, recommender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P3alphaRecommender: URM Detected 4326 (14.00 %) cold users.\n",
      "P3alphaRecommender: URM Detected 3694 (19.97 %) cold items.\n"
     ]
    }
   ],
   "source": [
    "from KNN.P3alphaRecommender import P3alphaRecommender\n",
    "\n",
    "P3alpha = P3alphaRecommender(URM_train)\n",
    "P3alpha.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ItemKNNSimilarityHybridRecommender: URM Detected 4326 (14.00 %) cold users.\n",
      "ItemKNNSimilarityHybridRecommender: URM Detected 3694 (19.97 %) cold items.\n",
      "EvaluatorHoldout: Processed 20444 ( 100.00% ) in 11.44 sec. Users per second: 1787\n"
     ]
    }
   ],
   "source": [
    "## hybrid Recommender semplice\n",
    "from KNN.ItemKNNSimilarityHybridRecommender import ItemKNNSimilarityHybridRecommender\n",
    "hybridrecommender = ItemKNNSimilarityHybridRecommender(URM_train, recommender.W_sparse, P3alpha.W_sparse)\n",
    "hybridrecommender.fit(alpha = 0.5)\n",
    "evaluator_validation = EvaluatorHoldout(URM_test, cutoff_list=[10])\n",
    "dict_scores = (evaluator_validation.evaluateRecommender(hybridrecommender))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rYQRvvikMJVN"
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "for user_id in target_users:\n",
    "    output.append((user_id, hybridrecommender.recommend(user_id, cutoff=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o5TyQsg9TAwz"
   },
   "outputs": [],
   "source": [
    "with open('submission.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"user_id\", \"item_list\"])\n",
    "    for row in output:\n",
    "      ranking = ''\n",
    "      for val in row[1]:\n",
    "        ranking = ranking + str(val) + ' '\n",
    "      writer.writerow([row[0], ranking[:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "sacajrota.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
